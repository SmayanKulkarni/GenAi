{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Here‚Äôs a general 500-word speech on **\"The Power of Positivity\":**\\n\\n---\\n\\n**The Power of Positivity**  \\n\\nGood morning, everyone.  \\n\\nToday, I want to talk about something that touches every aspect of our lives: the power of positivity. In a world filled with challenges, uncertainties, and setbacks, positivity is not just a state of mind‚Äîit is a transformative force that can shape our reality, enhance our well-being, and inspire others.  \\n\\nAt some point in life, we all face difficulties. Whether it‚Äôs a failed exam, a lost job opportunity, or even a strained relationship, life doesn‚Äôt always go as planned. But the way we respond to these challenges defines who we are. Positivity doesn‚Äôt mean ignoring problems or pretending everything is perfect. Instead, it‚Äôs about maintaining hope, finding solutions, and believing that better days are ahead.  \\n\\nScience supports the power of positivity. Research has shown that a positive outlook can lead to better physical health, lower stress levels, and improved mental resilience. When we focus on the good, even amidst adversity, we train our brains to seek opportunities rather than obstacles. This doesn‚Äôt just help us‚Äîit has a ripple effect, influencing those around us. A simple smile, a kind word, or a small act of gratitude can brighten someone‚Äôs day and create a cycle of positivity.  \\n\\nConsider some of the most inspiring individuals throughout history‚Äîpeople like Mahatma Gandhi, Nelson Mandela, and Malala Yousafzai. What made them extraordinary was not just their courage but their ability to maintain hope and optimism even in the face of unimaginable challenges. Their positivity didn‚Äôt just fuel their own perseverance; it inspired movements, united communities, and changed the world.  \\n\\nNow, let‚Äôs talk about how we can cultivate positivity in our daily lives. First, gratitude is a powerful tool. By acknowledging the things we‚Äôre thankful for‚Äîno matter how small‚Äîwe shift our focus away from what‚Äôs lacking to what‚Äôs abundant. Try starting or ending each day by listing three things you‚Äôre grateful for. You‚Äôll be amazed at how this simple habit can transform your perspective.  \\n\\nSecond, surround yourself with positive influences. The people we spend time with and the content we consume shape our thoughts and emotions. Seek out friends, mentors, or even books that uplift and inspire you. And don‚Äôt underestimate the power of self-talk. Be kind to yourself. Replace ‚ÄúI can‚Äôt do this‚Äù with ‚ÄúI‚Äôll give it my best shot.‚Äù  \\n\\nFinally, embrace challenges as opportunities for growth. Every setback teaches us something valuable. The next time you face a difficult situation, ask yourself, ‚ÄúWhat can I learn from this? How can I grow stronger?‚Äù You‚Äôll find that even the toughest moments carry seeds of positivity.  \\n\\nAs I conclude, I want to remind you that positivity is not a destination‚Äîit‚Äôs a journey. It‚Äôs a daily choice to see the silver lining, to believe in your potential, and to spread light in the lives of others. Let‚Äôs commit to being beacons of positivity, not just for ourselves but for those around us.  \\n\\nTogether, we can create a brighter, more hopeful world. Thank you.  \\n\\n---  \\n\\nWould you like me to tailor this speech for a specific purpose or audience?')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')\n",
    "loader\n",
    "text_documenets = loader.load()\n",
    "text_documenets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Transfer_Learning_for_Robust_Masked_Face_Recognition.pdf', 'page': 0, 'page_label': '1'}, page_content=\"Transfer Learning for Robust Masked Face \\nRecognition \\n \\n                Omar Adel Muhi                                             Mariem Farhat                                Mondher Frikha  \\n                        ATISP research unit                                                                ATISP research unit                                             ATISP research unit  \\n                  √âcole Nationale d‚Äô√âlectronique                                            Higher Institute of Computing                              √âcole Nationale d‚Äô√âlectronique \\n  et de T√©l√©communications de Sfax (ENET'Com)                                 and Multimedia of Gabes                       et de T√©l√©communications de Sfax (ENET'Com) \\n            omar.muhi@yahoo.com                                                        mariem.farhat@isimg.tn                                       mondher.frikha@enetcom.usf.tn \\nAbstract‚Äî Over the past three years, it has seen many \\noutbreaks of different coronavirus diseases around the world. One \\nof the ways of transmission of COVID 19 is air transport. This \\ntransmission occurs when humans breathe in droplets released by \\nan infected person by breathing, speaking, coughing or sneezing. \\nThe World Health Organization (WHO) has given orders to wear \\na face mask in the public places.  \\nDespite the impressive results achieved by deep learning \\nmethods in face recognition, the performance of these met hods \\ndeteriorates when wearing a mask.  The issue of masked face \\nrecognition is attracting more attention. \\nIn this work, a simple and effective method is proposed to deal \\nwith the recognition of people who wear a mask. We divide the \\nproblem into three phase s: feature detection, feature extraction \\nand recognition. The first stage of our model with MediaPipe Face \\nMesh automatically produces a segmentation of the masked area \\nas feature detection and several points for cropping the area of \\ninterest. Then, the second stage extracts the features gained using \\nResnet50. The selected network, based on ResNet -50, is modified \\nso that the third stage performs the classification process with Soft \\nMax as an redaction and activation function.  \\nTo train our system, we used t he Labeled Faces in the Wild \\n(LFW).  From the original facial recognition data set, a masked \\ncopy is generated using data augmentation, and the two data sets \\nare combined during the training process. Our model achieved a \\ntest accuracy of 98%. \\nKeywords‚Äîmasked face recognition, LFW, ResNet-50, SoftMax  \\nI. INTRODUCTION  \\nIn April 2020 there is a research has been published by CSIS \\nit discussed the matter of facial recognition system that has \\nabsolute precision in the ideal condition, reached to 99.97% of \\nrecognition accuracy level [1]  \\nMoreover, while the face recognition has reached an \\nunprecedented level in perfect conditions, this does not scale to \\ndaily and real-world conditions. In fact, Ageing, Low resolution, \\nlighting, facial covering, are the factors which subject under real \\nworld condition, they affect by accuracy of facial recognition \\ntechnology [2].  \\nGo back to our real life and for the last 3 years, people have \\nbeen wearing a mask to prevent covid-19 disease. The World \\nHealth Organization has proven that when we wear face masks, \\nit prevents the transmission of the Corona virus. This face mask \\ncovers Large part from face including the nose and mouth. This \\nis a special case of occlusion that makes the task of recognition \\nmore difficult. \\nThus, masked face recognition is considered as a crucial, \\nurgent and immediate challenge to defeat with new face \\nrecognition methods and systems. NIST [ 2] review ed the \\nperformance of FR algorithms before and after the COVID -19 \\npandemic. They evaluated existing (pre -pandemic) algorithms \\nafter they had been modified to deal with maskers.  The main \\nconclusion from this study is that the accuracy of the recognition \\nsystem drops from 100% to 96% when a person wears a face \\nmask. \\nResearchers have noted many difficult to deal with this issue. \\nFirstly, Most of the current advanced face recognition methods \\nare based on deep learning, which mainly  based on a large \\nnumber of tr aining samples. However, there is currently a \\nshortage of large, publicly available facial data set with masks \\n[3]. Second, because the mask covers a large part of the face \\nwhere there are abundant features such as the mouth and nose, \\nthe traditional facia l recognition algorithm may not work \\neffectively. Finally, it is difficult to determine which face is \\nwearing the mask.  \\nTo meet the challenge of wearing masks, It is necessary to \\nimprove existing facial recognition algorithms. \\nHence, we state a masked fac e recognition model that \\ndepends on deep transfer learning using ResNet50. \\n6th International Conference on Advanced Technologies\\nfor Signal and Image Processing - ATSIP'2022\\nMay 24-27, 2022, Canada-Tunisia\\nIVP-26\\n \\n978-1-6654-5116-1/22/$31.00 ¬©2022 IEEE\\n2022 6th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP) | 978-1-6654-5116-1/22/$31.00 ¬©2022 IEEE | DOI: 10.1109/ATSIP55956.2022.9805960\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on January 11,2025 at 09:33:02 UTC from IEEE Xplore.  Restrictions apply. \"),\n",
       " Document(metadata={'source': 'Transfer_Learning_for_Robust_Masked_Face_Recognition.pdf', 'page': 1, 'page_label': '2'}, page_content=\"The organization for the rest of the paper is as follows. \\nSection 2 reviews relevant previous work. Section 3 explains the \\nproposed model in detail. Section 4 describes the characteristics \\nof the dataset. Section 5 includes there reporters and analysis of \\nresults and Section 6 includes the possibilities and conclusions \\nof the next work. \\nII. RELATED WORKS \\nFace recognition (FR) has a long standing research topic that \\nrelated to computer vision . With deep learning techniques, the \\nmajority of FR systems have been transformed to implement \\ndeep learning models and accuracy has been greatly enhanced \\nto over 99.80% in just a few years approaching human \\nperformance in an unconstrained state  [4]. Hence, the face \\nrecognition tends to make far from satisfactorily to encounter \\nchallenges like large pose variation, low resolution facial \\nexpressions that are difference and conclusion. \\nOccluded face recognition (OFR) It considers one of the \\ndifficult problems because the researcher can't reach to the \\nknowledge of the conclude part, that could be in everywhere and \\nany form in the face image . While, masked face recognition \\n(MFR) is a specific case of the OFR where the occluded part of \\nthe face is known. Thus, the MFR task can be considered easier \\nto solve and over the past three years there has been a rapid \\ngrowth in the amount of research work in the field of MFR. \\nExhaustive surveys on FR [4]‚Äì[6][8] [9], OFR [2], [10][11] \\nand MFR [12] have been published. \\nAlthough it is an important part of recognition systems, the \\nproblem of occluded facial images, including masks, has not \\nbeen fully addressed. \\nSince the emergence of COVID-19, many works have been \\nsubmitted to solve the task of recognizing masked faces by using \\ndifferent methods that can be categorized into three groups. \\nThe first group is based on m atching based methods [1 1], \\n[16] Which attempts to compare the similarity between images \\nusing the matching process.  \\nThe face image is sampled at a number of points of the same \\nsize. The feature extraction is then applied to each patch. Finally, \\nthe match ing process is applied between the interface of the \\nprobe and the gallery . These methods treat MFR as a normal \\nface recognition problem and do not take into account damage \\nto the masked area and lack of texture features in the area around \\nthe mouth and nose. Moreover, the performance of these is very \\nsensitive to different sampling strategies. \\nThe second group is based on discarding methods [14] [15] \\n[16] which rejects the occluded parts completely to avoid \\ninefficient features, so they use only features extracted from the \\nvisible parts for recognition . This strategy become more \\ninteresting when dealing with masks because we already know \\nthe position of the occluded parts on the face.  \\nFor instance, Hariri [1 4] proposed a masked -aware face \\nfeature embedding By cropping the eyes and forehead, and then \\nusing a quantization -based clustering method on a pre -trained \\nVGG-16 model to extract deep features from unmasked areas. \\nThis approach makes the MFR applicable in real -time \\napplications because it handles only the most interesting part of \\nthe image.  \\nMasker recovery which aims to recover the covered parts of \\nthe images based on the images in the training in the third group \\n[17]. According to this strategy, it decreases the difficulties of \\nrecognition largely.  \\nFor example, System et al. [ 17] The second stage removes \\nthe mask and aggregates the affected region in fine detail while \\nretaining the global coherence of the face structure using a \\nGAN-based network after producing a binary segmentation of \\nthe mask region. \\nIn [18], the authors suggested drawing the closed segment \\nusing GAN integration with the previously i mproved CNN \\nrecognizer. With the goal, a set of identity -centric features are \\napplied to the recognizer as a moderation to enable colored faces \\nto converge towards the centers of their own identity. In this \\nway, their approach can take advantage of GAN for  \\nreconstruction and CNN for representation, while \\nsimultaneously addressing two difficult tasks, namely, face \\npainting and face recognition. \\nThe reconstructed faces are synthetic and their reliability \\ndepends on the quality of the data, network and training process \\nin addition, this strategy is not good and is not preferred for real-\\nworld applications because it is time -consuming. In our work, \\nwe set two challenges: (1) a robust system that can deal with \\nboth masked and non-masked faces that it could achieve a good \\nperformance in both evaluation sets, (2) a solution for the lack \\nof public masked dataset.  \\nIn order to reach these goals, we propose a system based on \\nocclusion discard strategy using a transfer learning approach.  \\nOn the one hand, The most advanced face recognition based \\non deep learning [19] [20] introduce several CNN architecture \\nthat already achieved a very good performance in the FR task. \\nOn the other hand, these architectures don‚Äôt require a huge \\nnumber of images to fine tuning them compared with a solution \\nfrom scratch [21]. Thus, we propose a system based on ResNet-\\n50 [22] to extract deep features from the non -occluded part of \\nthe masked faces. Nevertheless, we propose a data augmentation \\nthrough the generation of masked images using MediaPipe face \\nMesh [23] applied on the LFW dataset [24]. \\nIII. THE PROPOSED MODEL \\nIn this section, we will present the detailed structure for our \\nproposed model based on MediaPipe Landmark activation, \\nResnet-50 and SoftMax. \\nOur model is divided into three components Fig. 1: first, the \\nface detection using Media Pipe face Mesh which estimate 468 \\n3D face landmarks on devices in real -time. Second,  the \\nsegmentation component which extract the region of interest \\nthat contains most important features in the non -occluded part \\nof the face. Finally, the ResNet50 component which performs \\nthe feature extraction and the classification process using the \\nSoftMax activation function. \\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on January 11,2025 at 09:33:02 UTC from IEEE Xplore.  Restrictions apply. \"),\n",
       " Document(metadata={'source': 'Transfer_Learning_for_Robust_Masked_Face_Recognition.pdf', 'page': 2, 'page_label': '3'}, page_content=\" \\nFig.  1: The Proposed System \\nA. Face Detection \\nFirst, the masked face is detected using a landmark detector.  \\nThere are many landmark face detection, like Dlib (68, 85) [25] \\nand the Media Pipe Face Mesh [23] Fig.2. the Media pipe \\nlandmark have two advantages; first it is more suitable for a real \\ntime application, second it provides  a huge number of points \\naround the face. These landmarks are used for face correction \\nbefore feature extraction and for data augmentation in an offline \\nprocess to generate new masked images. \\n \\nFig.  2: Face landmarks:  The black nu mber dots represent the 468 landmarks \\nin 3D [28]. \\nB. Segmentation  \\nIn order to crop the region of interest and maintain only the \\nnon-occluded region, we set four static points of this area using \\nextracted vertex of the detected mesh in the last component.  \\nTo get the correct image dimensions, we multiply the point \\nby the length and width. Using Region of interest (ROI) function \\nwe cover the area with a rectangle. We tried to extract this region \\nusing 10, 8, 6 and 4 points. After several attempts, we retain   \\nonly 4 points in order reduce the computational time. We make \\na full mask for each covering image and make it black, except \\nfor the selected area, which will be in white. Finally, we apply \\nan AND operation to this image with the original morphological \\nimage and we got a picture of the selected part sized 100*100 \\nFig. 3. \\n \\nFig.  3: Landmark detection and a Region of Interest cropped \\nC. Feature-Extractor and Classifier \\n \\nThey proven that ResNet50 [ 26] reached to the best result \\nbecause it has used as feature extractor. ResNet50 represents \\ndeep learning based on residual learning [22]. Also ResNet.50 \\nwith about 50 layers, starting involution layer, then ending with \\nconnection layers. It contains 16 residual bottleneck block which \\neveryone joins 3 layers. \\nThe SoftMax loss and its variants are widely used as \\nobjectives for face recognition [27]. We used the following \\ndefinition of the SoftMax function: \\n  ùë†ùëúùëìùë°ùëöùëéùë•(ùëßùëñ) =\\nexp\\u2061(ùëßùëñ)\\n‚àë ùëßùëóùëó\\n    (1) \\nWhere ùëßùëñ represents values from  the neurons of the output \\nlayer. The exponential works as a non -linear function.  Later \\nthese values are divided by the sum of the exponential values in \\norder to normalize them and then convert them to probabilities. \\nSoftMax is used in polynomial logistic regression and is \\noften used as the redaction and activation function of a neural \\nnetwork to normalize the network's output to a probability \\ndistribution on expected output classes , based on the loss \\nselection axiom, a logistic generalization that operates on \\nmultiple dimensions [29]. \\nIV. EXPERIMENTATION RESULTS \\nA. Dataset \\nThe first step is to create a training dataset containing pre -\\nprocessed input images for mask face recognition. We apply a \\nmask on each image of the LFW dataset [24] in offline using a \\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on January 11,2025 at 09:33:02 UTC from IEEE Xplore.  Restrictions apply. \"),\n",
       " Document(metadata={'source': 'Transfer_Learning_for_Robust_Masked_Face_Recognition.pdf', 'page': 3, 'page_label': '4'}, page_content='tool Mask The Face1. We generate for each person five masked \\nimages. The total number of images without the mask 20 photos \\n* 100 people = 2000 photos. \\nAnd we split our dataset in three categories as following:  \\n1- Number of images used in training A (unmasked) = \\n2000 \\n2- Number of images used in training B (masked and \\nunmasked) = 2500 \\n3- The number of images used in the test (only masked \\nfaces) = 500 \\nDuring the training, people are chosen at random. \\nB. Implementation Details \\nAll the experimental trials were conducted on a computer HP \\nsever has equipped by processor (2 GHz), Core ( TM) i7. All \\nimplementation was done using Python 3.8.  \\nThe initial learning rate is set to 0.1. We have tried 10, 50, 100 epochs, \\nand stop training at 100  epochs. For loss function pool from 20.7 to \\n0.28.  \\nC. Results  \\nWe evaluate our approach using the following metrics: \\nùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ =\\n(ùëáùëÉ+ùëáùëÅ)\\n(ùëáùëÉ+ùëáùëÅ)+(ùëáùëÅ+ùêπùëÅ)  (2) \\nWe have a lot of symbols, one of them is TP for True Positive, \\nalso FP is abbreviated to False Positive and FN for False \\nnegative sample.  \\nThe researchers [ 28] build their approach based on deep \\ntransfer learning in order to enhance image of classification \\naccuracy, whereas t he last is not acceptable. We are going to \\ndiscuss the achieved results for the joint use of ResNet and \\nsoftmax classifier in this work. We will look also for the results \\nusing two methods of face detection Dlib, and Media Pipe Face \\nMesh as shown in the Table (1).  \\nFig. 4 illustrates the achieved percentage for the training \\naccuracy evolution. We have achieved an accuracy around \\n100% for each scenario.  \\n \\nFig.  4: Accuracy function on the training set \\n \\n1 https://github.com/aqeelanwar/MaskTheFace \\nTABLE I. RESULTS FOR DIFFERENT SCENARIOS \\nScenario Epoch Detection \\nmodel \\nTrain \\nImage \\nTest \\nImage \\nTrain \\nAcc \\n   (%) \\nTest \\nAcc \\n(%) \\nScenario \\n1 \\n100 MediaPipe faces \\nwithout \\nmask \\nMasked \\nFaces \\n100 97.34 \\nScenario \\n2 \\n50 MediaPipe faces \\nwithout \\nmask \\nMasked \\nFaces \\n100 97 \\nScenario \\n3 \\n100 MediaPipe faces \\nwithout \\nmask \\nMasked \\nFaces \\n100 98.3 \\nScenario \\n4 \\n50 MediaPipe faces \\nwithout \\nmask \\nMasked \\nFaces \\n100 98.1 \\nScenario \\n5 \\n100 Dlib faces \\nwithout \\nmask \\nMasked \\nFaces \\n98 96 \\nScenario \\n6 \\n50 Dlib faces \\nwithout \\nmask \\nMasked \\nFaces \\n95 95.2 \\nScenario \\n7 \\n100 Dlib faces \\nwithout \\nmask + \\nMasked \\nFaces \\nMasked \\nFaces \\n100 97.1 \\nScenario \\n8 \\n50 Dlib faces \\nwithout \\nmask + \\nMasked \\nFaces \\nMasked \\nFaces \\n100 97 \\n \\nTable I. presents a comparison between different scenarios \\nwhere both masked and unmasked faces are used independently \\nand jointly. Using unmasked images and masked images for the \\ntraining of the considered face recognition model, MediaPipe, \\nResNet-50 and SoftMax, achieve a very high verification \\nperformance 98.3% accuracy. The verification performances of the \\nconsidered models ar e substantially degraded when only unmasked \\nface images are considered. \\nWe make also the observation that the Media pipe Landmark  \\nreturn better recognition accuracy than Dlib in all scenarios. \\nV. COMPARISON WITH RELATED WORKS \\nThe work presented in [ 3] used the same data sets, the LFW \\npseudo-disguised data set. The authors [ 3] achieved test \\naccuracy ranging from 50% to 95%. In the presented work, the \\naccuracy of the test was tested with a ResNet -50 classifier. In \\nthis work, we report an accuracy of between 97% and 98%. \\nVI. CONCLUSION AND FUTURE WORKS \\nIn this work, we seek to investigate the performance of face \\nrecognition models on mask ed faces. The proposed model \\nsuggests that feature extraction has used ResNet50. It \\nrepresents models in transfer learning. The Res Net with Soft \\nmax classifier in LFW database achieved 98.3% testing \\naccuracy. The result after making the comparison has bee n \\napplied with same works. Also the proposed model has been \\nachieved the successful in the testing accuracy. Show our model \\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on January 11,2025 at 09:33:02 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'Transfer_Learning_for_Robust_Masked_Face_Recognition.pdf', 'page': 4, 'page_label': '5'}, page_content='tested on these datasets shows better recognition rates. So \\nResNet-50 gives better accuracy for simple recognition of \\nmasked faces. In subsequent work, it is of paramount \\nimportance to enhance and extend our work to address the case \\nof various extreme masks for face recognition. \\nVII. REFERENCES \\n[1] P. Grother, M. Ngan, and K. Hanaoka, ‚ÄúOngoing Face \\nRecognition Vendor Test (FRVT) Part 1: Verification,‚Äù 2020. \\n[2] D. Zeng, R. Veldhuis, and L. Spreeuwers, ‚ÄúA survey of \\nface recognition techniques under occlusion,‚Äù IET Biometrics, \\nvol. 10, no. 6, pp. 581‚Äì606, 2021. \\n      [3] Z. Wang et al., ‚ÄúMasked face recognition dataset and \\napplication,‚Äù arXiv, pp. 1‚Äì3, 2020. \\n[4] M. Wang and W. Deng, ‚ÄúDeep face recognition: A \\nsurvey,‚Äù Neurocomputing, pp. 1‚Äì31, 2020. \\n[5] U. Jayaraman, P. Gupta, S. Gupta, G. Arora, and K. \\nTiwari, ‚ÄúRecent development in face recognition,‚Äù \\nNeurocomputing, vol. 408, pp. 231‚Äì245, 2020. \\n[6] G. Guo and N. Zhang, ‚ÄúA survey on deep learning \\nbased face recognition,‚Äù Comput. Vis. Image Underst., vol. 189, \\nno. July, p. 102805, 2019. \\n[7] S. Madhavan and N. Kumar, Incremental methods in \\nface recognition: a survey, vol. 54, no. 1. Springer Netherlands, \\n2021. \\n[8] M. O. Oloyede, G. P. Hancke, and H. C. Myburgh, ‚ÄúA \\nreview on face recognition systems: recent approaches and \\nchallenges,‚Äù Multimed. Tools Appl ., vol. 79, no. 37 ‚Äì38, pp. \\n27891‚Äì27922, 2020. \\n[9] B. Lahasan, S. L. Lutfi, and R. San -Segundo, ‚ÄúA \\nsurvey on techniques to handle face recognition challenges: \\nocclusion, single sample per subject and expression,‚Äù Artif. \\nIntell. Rev., vol. 52, no. 2, pp. 949‚Äì979, 2019. \\n[10] L. Ouannes, A. Ben Khalifa, and N. E. Ben Amara, \\n‚ÄúDeep Learning vs Hand-Crafted Features for Face Recognition \\nunder Uncontrolled Conditions,‚Äù 2019 Int. Conf. Signal, Control \\nCommun. SCC 2019, pp. 185‚Äì190, 2019. \\n[11] D. Zeng, R. Veldhuis, and L. Spreeuwers, ‚ÄúA survey of \\nface recognition techniques under occlusion,‚Äù arXiv, pp. 1 ‚Äì23, \\n2020. \\n[12] A. Alzu, F. Albalas, T. Al-hadhrami, L. B. Younis, and \\nA. Bashayreh, ‚ÄúMasked Face Recognition Using Deep \\nLearning\\u202f: A Review,‚Äù 2021. \\n[13] V. Aswal, O. Tupe, S. Shaikh, and N. N. Charniya, \\n‚ÄúSingle Camera Masked Face Identification,‚Äù pp. 57‚Äì60, 2021. \\n[14] W. Hariri, ‚ÄúEfficient Masked Face Recognition \\nMethod during the COVID-19 Pandemic,‚Äù 2020. \\n[15] Q. Wang, T. Wu, H. Zheng, and G. Guo, ‚ÄúHierarchical \\nPyramid Diverse Attention Networks for Face Recognition,‚Äù pp. \\n8326‚Äì8335. \\n[16] Y. Li, K. Guo, Y. Lu, and L. Liu, ‚ÄúCropping and \\nattention based approach for masked face recognition,‚Äù Appl. \\nIntell., 2021. \\n[17] N. Ud Din, K. Javed, S. Bae, and J. Yi, ‚ÄúA Novel GAN-\\nBased Network for Unmasking of Masked Face,‚Äù IEEE Access, \\nvol. 8, pp. 44276‚Äì44287, 2020. \\n[18] S. Ge, C. Li, S. Zhao, and D. Zeng, ‚ÄúOccluded face \\nrecognition in the wild by identity -diversity inpainting,‚Äù IEEE \\nTrans. Circuits Syst. Video Technol., vol. 30, no. 10, pp. 3387‚Äì\\n3397, 2020. \\n[19] Y. Taigman, M. A. Ranzato, T. Aviv, and M. Park, \\n‚ÄúDeepFace\\u202f: Closing the Gap to Human -Level Performance in \\nFace Verification.‚Äù \\n[20] F. Schroff and J. Philbin, ‚ÄúFaceNet: A Unified \\nEmbedding for Face Recognition and Clustering.‚Äù \\n[21] A. Brodzicki, M. Piekarski, and D. Kucharski, \\n‚ÄúTransfer Learning Methods as a New Approach in Computer \\nVision Tasks with Small Datasets Introduction to Transfer \\nLearning Methods,‚Äù vol. 45, no. 3, 2020. \\n[22] K. He and J. Sun, ‚ÄúDeep Residual Learning for Image \\nRecognition,‚Äù pp. 1‚Äì9. \\n[23] C. Lugaresi et al., ‚ÄúMediaPipe: A Framework for \\nBuilding Perception Pipelines.‚Äù \\n[24] G. B. Huang et al., ‚ÄúLabeled Faces in the Wild\\u202f: A \\nDatabase forStudying Face Recognition in Unconstrained \\nEnvironments To cite  this version\\u202f: HAL Id\\u202f: inria -00321923 \\nLabeled Faces in the Wild\\u202f: A Database for Studying Face \\nRecognition in Unconstrained Environments,‚Äù 2008. \\n[25] S. Sharma, K. Shanmugasundaram, and S. K. \\nRamasamy, ‚ÄúFAREC - CNN Based Efficient Face Recognition \\nTechnique using Dlib,‚Äù no. 978, pp. 192‚Äì195, 2016. \\n[26] M. Loey, G. Manogaran, M. Hamed, N. Taha, N. \\nEldeen, and M. Khalifa, ‚ÄúA hybrid deep transfer learning model \\nwith machine learning methods for face mask detection in the \\nera of the COVID -19 pandemic,‚Äù Measur ement, vol. 167, no. \\nJuly 2020, p. 108288, 2021. \\n[27] X. An et al., ‚ÄúPartial FC\\u202f: Training 10 Million Identities \\non a Single Machine,‚Äù pp. 1445‚Äì1449. \\n      [28] B. Mandal, ‚ÄúMasked Face Recognition using ResNet-\\n50,‚Äù 2012. \\n      [29] A. GravesIan, G. Yoshua,  B. Geoffrey, H. Yann, L. \\nAndrew, N. Demis, H. David, S. Fei -Fei Li. \\nhttps://en.wikipedia.org/wiki/Softmax_function  \\nOctober 2020 (UTC). \\n \\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on January 11,2025 at 09:33:02 UTC from IEEE Xplore.  Restrictions apply. ')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a pdf file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('Transfer_Learning_for_Robust_Masked_Face_Recognition.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web based loader\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_paths=(\"https://www.digitalocean.com/resources/articles/ai-blogs\",),\n",
    "                       bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "                           class_ = (\"post-title\", \"post-header\", \"post-content\")\n",
    "                       )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.digitalocean.com/resources/articles/ai-blogs'}, page_content='')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web = loader.load()   \n",
    "web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query = \"1605.08386\", load_max_docs =2).load()  ## The query parameter is the paper number\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
